{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree Ensembles:In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
    "\n",
    "Random Forest: A random forest is a meta estimator that fits a number of decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "Extra Trees: In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias.\n",
    "\n",
    "Gradient Boosting: Boosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set. The gradient boosting algorithm begins by training a decision tree in which each observation is assigned an equal weight. After evaluating the first tree, we increase the weights of those observations that are difficult to fit and lower the weights for those that are easy to fit. The second tree is therefore grown on this weighted data. Here, the idea is to improve upon the predictions of the first tree.\n",
    "\n",
    "Light GBM: Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithms grow level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n",
    "\n",
    "XG Boost: XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\n",
    "\n",
    "XG Boost RF XGBoost RF is an optimized distributed gradient boosting library combining the features of random forests with gradient boosting.\n",
    "\n",
    "Cat Boost: “CatBoost” name comes from two words “Category” and “Boosting”. For fitting a model on some data generally, we are required to convert categorical data into the numerical format. using several pre-processing methods like “label encoding”, “one hot encoding” and others. But catboost can use categorical features directly and is scalable in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import missingno as msno\n",
    "import statsmodels.api as sm\n",
    "#import tensorflow as tf\n",
    "#imp\n",
    "df = pd.read_csv(r\"C:\\Users\\Hamza\\Desktop\\P2P ortak dosya\\Makine Öğrenmesi\\Solar1\\solar_predict.xls\")   #dataframe determined\n",
    "df.columns=[\"Date\",\"Time\",\"Power\",\"Load\",\"Temp\"]   #the columns in our dataframe\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "df.index=df.iloc[:,0]\n",
    "df.index = pd.to_datetime(df.index)  #convert to date\n",
    "df.drop(['Load'], axis=1, inplace=True)   #I will just look at PV Generation\n",
    "df['MonthOfYear'] = df.index.strftime('%m').astype(int)\n",
    "df['DayOfYear'] = df.index.strftime('%j').astype(int)\n",
    "df['WeekOfYear'] = df.index.strftime('%U').astype(int)\n",
    "df.index=df.iloc[:,1]\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df['TimeOfDay'] = df.index.hour\n",
    "from pytz import timezone\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.models import Range1d\n",
    "#from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "#from tensorflow.keras.models import Sequential,load_model\n",
    "#from tensorflow.keras.optimizers import SGD, Adam\n",
    "#from tensorflow.keras.models import Sequential\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge ,Lasso ,ElasticNet, LassoLarsIC\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold,RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale, LabelEncoder  \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor \n",
    "from sklearn.svm import SVR\n",
    "#from catboost import CatBoostRegressor\n",
    "#from lightgbm import LGBMRegressor\n",
    "#from xgboost import XGBRegressor\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "df = pd.read_csv(r\"C:\\Users\\Hamza\\Desktop\\P2P ortak dosya\\Makine Öğrenmesi\\Solar1\\solar_predict.xls\")   #dataframe determined\n",
    "df.columns=[\"Date\",\"Time\",\"Power\",\"Load\",\"Temp\"]   #the columns in our dataframe\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "df.index=df.iloc[:,0]\n",
    "df.index = pd.to_datetime(df.index)  #convert to date\n",
    "df.drop(['Load'], axis=1, inplace=True)   #I will just look at PV Generation\n",
    "df['MonthOfYear'] = df.index.strftime('%m').astype(int)\n",
    "df['DayOfYear'] = df.index.strftime('%j').astype(int)\n",
    "df['WeekOfYear'] = df.index.strftime('%U').astype(int)\n",
    "df.index=df.iloc[:,1]\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df['TimeOfDay'] = df.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ae4daf3d202b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "kf = KFold(shuffle=True, random_state=19)\n",
    "\n",
    "scores = []\n",
    "rmse = []\n",
    "mae = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = DummyRegressor(strategy='mean').fit(X_train, y_train)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n",
    "    mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "    \n",
    "print('Mean R2 Score:', round(np.mean(scores), 5))\n",
    "print('Mean RMSE:', round(np.mean(rmse), 5))\n",
    "print('Mean MAE:', round(np.mean(mae), 5))\n",
    "\n",
    "%%time\n",
    "\n",
    "scores = []\n",
    "rmse = []\n",
    "mae = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    dtmodel = DecisionTreeRegressor(random_state=19).fit(X_train, y_train)\n",
    "    scores.append(dtmodel.score(X_test, y_test))\n",
    "    rmse.append(np.sqrt(mean_squared_error(y_test, dtmodel.predict(X_test))))\n",
    "    mae.append(mean_absolute_error(y_test, dtmodel.predict(X_test)))\n",
    "    \n",
    "print('Mean R2 Score:', round(np.mean(scores), 5))\n",
    "print('Mean RMSE:', round(np.mean(rmse), 5))\n",
    "print('Mean MAE:', round(np.mean(mae), 5))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "trees = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=19), 'ExtraTrees': ExtraTreesRegressor(random_state=19),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=19), 'LightGBM': LGBMRegressor(random_state=19),\n",
    "    'XGBoost': XGBRegressor(random_state=19), 'XGBoostRF': XGBRFRegressor(random_state=19), \n",
    "    'CatBoost': CatBoostRegressor(random_state=19, silent=True)\n",
    "}\n",
    "\n",
    "performance = {'rmse':[], '100* r2':[], 'mae':[]}\n",
    "for name, model in trees.items():\n",
    "    scores = []\n",
    "    rmse = []\n",
    "    mae = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "        scores.append(100*model.score(X_test, y_test))\n",
    "        rmse.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n",
    "        mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "    performance['100* r2'].append(np.mean(scores))\n",
    "    performance['rmse'].append(np.mean(rmse))\n",
    "    performance['mae'].append(np.mean(mae))\n",
    "\n",
    "fig = px.bar(pd.DataFrame(performance, index=trees.keys()), barmode='group', title='Model Comparison')\n",
    "fig.show()\n",
    "\n",
    "feat_imp = {\n",
    "    k: trees[k].feature_importances_ for k, v in trees.items()\n",
    "}\n",
    "feat_imp['DecisionTree'] = dtmodel.feature_importances_\n",
    "feat_imp = pd.DataFrame(feat_imp)\n",
    "\n",
    "feat_imp /= feat_imp.sum()\n",
    "feat_imp.index = feats\n",
    "\n",
    "fig, ax= plt.subplots(figsize=(20, 6))\n",
    "fig.suptitle('Feature Importance', fontsize=18)\n",
    "pd.DataFrame(feat_imp).plot.bar(ax=ax, color=sns.color_palette(\"summer\", 8))\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.scatterplot(y_test,y_pred)\n",
    "\n",
    "plt.plot(y)\n",
    "plt.plot(regressor.predict(findata))\n",
    "\n",
    "regressor.score(findata,y)\n",
    "\n",
    "# https://www.kaggle.com/soheilarouhi/solar-radiation-prediction\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16,8)\n",
    "months = np.arange(9,13)\n",
    "rad_vs_month=data.loc[:, ['Radiation', 'month']].groupby('month').sum()\n",
    "rad_vs_month.plot()\n",
    "plt.xticks(months,['September', 'October', 'November', 'December'], rotation=30)\n",
    "plt.show()\n",
    "\n",
    "########################################## Exploratory Data Analysis ###################################################\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=data['Temperature'],y=data['Radiation'])\n",
    "plt.xlabel('Temperature (F)')\n",
    "plt.ylabel('Radiation (W/m2)')\n",
    "plt.title('Radiation versus Temperature')\n",
    "plt.show()\n",
    "\n",
    "############################################ Building Random Forest model  #############################################\n",
    "rf=RandomForestRegressor()\n",
    "\n",
    "# finding the best parameters by RandomizedSearchCV\n",
    "param_rf = {'bootstrap':['False'], 'n_estimators': [500], \"max_depth\": [20], \"max_features\": np.arange(3,5).tolist(), \"min_samples_leaf\": np.arange(80,100).tolist(), \"criterion\": [\"mse\"]}\n",
    "rf_cv=RandomizedSearchCV(rf, param_rf, cv=2)\n",
    "\n",
    "# Extract the best estimator\n",
    "rf_cv.fit(X, np.ravel(y))\n",
    "\n",
    "best_model = rf_cv.best_estimator_\n",
    "print(best_model)\n",
    "y_pred_rf=best_model.predict(X_validation)\n",
    "Result['Predicted_Radiation_RF']=y_pred_rf\n",
    "rmse = np.sqrt(mean_squared_error(y_validation, y_pred_rf))\n",
    "print(\"Root Mean Squared Error RF: {}\".format(rmse))\n",
    "\n",
    "################################################ Build XGBoost model ###################################################\n",
    "\n",
    "xgb = XGBRegressor(max_depth=35, random_state=42, n_estimators=1500, learning_rate=0.005, booster='gbtree', objective='reg:squarederror', min_child_weight=0.1, silent=1, n_jobs=10)\n",
    "\n",
    "xgb.fit(X, y.values.ravel())\n",
    "y_pred=xgb.predict(X_validation)\n",
    "\n",
    "predicted=pd.DataFrame(y_pred)\n",
    "Result['Predicted_Radiation_XGBoost']=y_pred\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_validation, y_pred))\n",
    "print(\"Root Mean Squared Error XGBoost: {}\".format(rmse))\n",
    "\n",
    "feat_imp = pd.Series(xgb.feature_importances_, index= X.columns).sort_values(ascending=True)\n",
    "feat_imp.plot(kind='barh', title='Feature Importances XGBoost') # note: there in no feature importance for lgbm\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.show()\n",
    "\n",
    "######################################### Build lightGB model ####################################################\n",
    "X_lgb = X.values\n",
    "\n",
    "params = {\n",
    "      'num_leaves': 700,\n",
    "      'min_child_weight': 0.34,\n",
    "      'feature_fraction': 0.979,\n",
    "      'bagging_fraction': 0.818,\n",
    "      'min_data_in_leaf': 700,\n",
    "      'objective': 'regression',\n",
    "      'max_depth': 40,\n",
    "      'learning_rate': 0.1,\n",
    "      \"boosting_type\": \"gbdt\",\n",
    "      \"bagging_seed\": 11,\n",
    "      \"metric\": 'rmse',\n",
    "      \"verbosity\": -1,\n",
    "      'reg_alpha': 0.0001,\n",
    "      'reg_lambda': 2.9,\n",
    "      'random_state': 666,\n",
    "    }\n",
    "\n",
    "lgb_train = lgb.Dataset(X.values, label=y_lgb.values)\n",
    "# Train LightGBM model\n",
    "m_lgb = lgb.train(params, lgb_train, 400)\n",
    "y_pred_lgb=m_lgb.predict(X_validation)\n",
    "#print(np.round(y_pred_lgb[0], 6))\n",
    "\n",
    "Result['Predicted_Radiation_LGB']=y_pred_lgb\n",
    "print(Result.loc[:, ['Predicted_Radiation_XGBoost', 'Radiation']].head())\n",
    "\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_validation, y_pred_lgb))\n",
    "print(\"Root Mean Squared Error LGBM: {}\".format(rmse_lgb))\n",
    "\n",
    "######################################### Performance comparison #######################################################\n",
    "Result=Result.rename({'Radiation':'Radiation_observed'}, axis=1)\n",
    "Result.plot(figsize=(20,12))\n",
    "plt.ylabel('Radiation(W/m2)')\n",
    "plt.title('Performance of different models in prediction of Radiation for 30 and 31 December 2016')\n",
    "plt.xlabel('Date and Time')\n",
    "plt.show()\n",
    "\n",
    "g = sns.jointplot(x=\"Radiation\", y=\"Temperature\", data=df)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('Temp vs. Radiation')\n",
    "\n",
    "# https://www.kaggle.com/plarmuseau/pysolar-dataset-exploration-and-r99-88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
